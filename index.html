<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Professional Portfolio Website">
    <title>Likhithsainadhreddy Satti</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Header Section -->
    <header>
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#about">About</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <!-- Home Section -->
    <section id="home">
        <h1>Likhithsainadhreddy Satti</h1>
        <p>Cincinnati, OH| +1 (513)-879-4177 | Likhit99.com@gmail.com | https://www.linkedin.com/in/likhith-reddy-satti/</p>
    </section>

    <!-- About Section -->
    <section id="about">
        <h2>About Me</h2>
        <p>Motivated and detail-oriented software and data engineer with hands-on experience in full-stack web development, data
            pipelines, and scalable application development. Adept at problem-solving, debugging, and collaborating in agile environments to
            deliver reliable, efficient solutions. Seeking to contribute my skills to a challenging position that leverages my experience in
            software engineering and data engineering.</p>
    </section>

    <!-- Experience Section -->
    <section id="experience">
        <h2>Experience</h2>
        <div class="experience-item">
            <h3>Technocentra, USA</h3>
            <p>Data Scientist Intern - Dec 2024 to Feb 2025</p>
            <ul>
                <li>•	Machine Learning & Model Development:
Designed and implemented supervised, unsupervised, and deep learning models using Python frameworks such as Scikit-learn and TensorFlow, improving risk assessment accuracy by 15%.
Developed predictive models that support real-time underwriting decisions and streamline risk evaluation.
•	Data Pipeline & Integration:
Collaborated with data engineers to build end-to-end data pipelines using PySpark and SQL, reducing data processing time by 25%.
Automated data cleansing processes to ensure high-quality inputs for modeling.
•	Stakeholder Engagement & Reporting:
Presented technical insights and model outcomes through interactive dashboards and detailed reports to inform strategic business decisions.
Actively engaged with internal and external stakeholders, ensuring adherence to data privacy regulations and compliance standards.
</li>
            </ul>
        </div>
         <h3>University of Cincinnati, USA</h3>
            <p>Event manager  - Aug 2023 to Dec 2024</p>
            <ul>
                <li>•	Planned and executed 50+ corporate and tech conferences (500–5000 attendees), using event management software (Cvent, Bizzabo, Eventbrite) to streamline registration and logistics.
•	Managed budgets up to $500K, utilizing Excel and Power BI for cost analysis, reducing expenses by 15% while optimizing vendor contracts.
•	Automated event scheduling and attendee tracking using Python scripts and SQL databases, improving data accuracy and reducing manual work by 30%.
•	Led digital marketing campaigns (Google Ads, HubSpot, Mailchimp), increasing event registrations by 25% and engagement rates by 40%.
•	Integrated AI-powered chatbots and RFID technology for real-time attendee support and check-ins, reducing wait times by 50% and enhancing user experience.
</li>
            </ul>
        </div>
         <h3>Aquaconnect, India</h3>
            <p>Senior Data Analyst - Jan 2022 to Jul 2023</p>
            <ul>
                <li>•	Data Reporting & Analysis:
Led the automation of complex reporting routines by designing robust Excel macros and enhancing workbook formulas, thereby reducing reporting time by 30%.
Conducted extensive statistical analysis to identify trends, providing actionable insights for risk management and policy adjustments.
•	User Acceptance Testing & Compliance:
Played a key role in the user acceptance testing (UAT) of new data management systems, ensuring the accuracy of rate tables and regulatory compliance.
Coordinated with IT and compliance teams to validate system changes and implement corrective measures based on testing results.
•	Data Extraction & Management:
Developed complex SQL queries to extract, clean, and analyze large datasets, supporting critical decision-making processes in the insurance domain.
</li>
            </ul>
        </div>
         <h3>Fresh Bags,India</h3>
            <p>Data Science Associate - Jun 2020 to Dec 2021</p>
            <ul>
                <li>•	Predictive Model Development:
Built and fine-tuned predictive models that were integrated into the company’s underwriting systems, contributing to a 10% reduction in claim variability.
Employed statistical techniques and data visualization tools to transform raw data into comprehensible insights for business stakeholders.
•	Process Automation & Data Workflow Optimization:
Streamlined ETL processes by automating data extraction and transformation tasks, leading to a 20% increase in processing efficiency.
Worked closely with software engineers to ensure seamless integration between newly developed algorithms and existing IT infrastructure.
•	Collaborative Project Management:
Assisted in the planning and execution of multiple cross-departmental projects focused on data compliance and risk assessment improvements.
Maintained documentation of methodologies and best practices in line with industry standards and internal policies.
</li>
            </ul>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects">
        <h2>Projects</h2>
        <div class="project-item">
            <h3>A multi-objective optimization Method of Initial Virtual Machine Fault-Tolerant Placement For Star Topological Data
                Centers of Cloud Systems (NOTE: The project has been published in UTS By Bharath University)</h3>
            <p>• The objective of this project is to develop a multi-objective optimization method for the initial placement of virtual
                machines (VMs) in star topological data centers within cloud computing systems. The primary goal is to enhance fault
                tolerance while considering other important factors.
                • Star topological data centers are common in cloud systems, but they are susceptible to single points of failure.
                Therefore, optimizing the initial placement of VMs is critical for improving fault tolerance and overall system
                reliability.</p>
            <h3>Smart ATM Pin Recovery</h3>
            <p>• Offer users an easy and user-friendly method to recover their ATM PINs without the need to visit a physical bank
                branch or endure a lengthy and cumbersome process.
                • The system prompts the user to provide additional verification details, such as account number, social security number,
                or answers to security questions to ensure that the requestor is the legitimate account holder.</p>
            <h3>Claims Management System Enhancement</h3>
            <p>• Led the redesign and implementation of the company's claims management platform, enhancing user experience
                through improved API integration, resulting in a 20% increase in processing efficiency.</p>
            <h3>Automated Deployment Pipeline</h3>
            <p>• Developed and implemented an automated deployment pipeline using GitHub and TFS, reducing deployment time by
                30% and minimizing errors during production releases.</p>
            <h3>Interactive Web Application Deployment on AWS EC2</h3>
            <p>Developed and deployed a full-stack interactive web application on an Amazon EC2 instance. The project involved setting up a web server, building a user registration system, handling authentication, and storing/displaying user data.
Key Highlights:
Launched and configured a publicly accessible EC2 instance on AWS.
Installed and set up Apache2 with WSGI and Python3 to run a Flask-based web server.
Created a multi-page web application that:
Registers users with username, password, and basic personal information.
Stores and retrieves user information with login functionality.
Displays user data on successful login.
Implemented file upload functionality (extra credit):
Allowed users to upload a .txt file (Limerick.txt).
Counted total words in the uploaded file and displayed results.
Enabled download link and data persistence on re-login.
Managed version control and collaboration via GitHub.
Developed and deployed a full-stack interactive web application on an Amazon EC2 instance. The project involved setting up a web server, building a user registration system, handling authentication, and storing/displaying user data. Key Highlights: Launched and configured a publicly accessible EC2 instance on AWS. Installed and set up Apache2 with WSGI and Python3 to run a Flask-based web server. Created a multi-page web application that: Registers users with username, password, and basic personal information. Stores and retrieves user information with login functionality. Displays user data on successful login. Implemented file upload functionality (extra credit): Allowed users to upload a .txt file (Limerick.txt). Counted total words in the uploaded file and displayed results. Enabled download link and data persistence on re-login. Managed version control and collaboration via GitHub.
Skills: AWS EC2 · Flask · Python 3 · Apache2 Web Server · Linux Server Administration · HTML/CSS · File Handling · User Authentication & Session Management · Data Persistence · Version Control · Networking & Public Web Access</p>
            <h3>Automated Text Processing in Docker</h3>
            <p>Built a lightweight Docker container to automate text file processing using a custom Python script. The container processes two input text files mounted at runtime, analyzes their contents, and outputs results to a specified directory, demonstrating containerization, automation, and scripting proficiency.

Key Features:
Used a minimal base image (Alpine Linux) to create a lightweight Docker container.
Installed Python in the container to execute a custom script automatically on launch.
Script capabilities included:
Listing all .txt files in /home/data
Counting words in each file (IF.txt and Limerick-1.txt)
Calculating total word count across files
Identifying the top 3 most frequent words in IF.txt with word counts
Retrieving the host machine's IP address
Writing all outputs to /home/output/result.txt
Configured the container to:
Run automatically upon execution
Print output to the console from result.txt
Exit cleanly after execution
Shared project image (.tar file) and source code via GitHub
Built a lightweight Docker container to automate text file processing using a custom Python script. The container processes two input text files mounted at runtime, analyzes their contents, and outputs results to a specified directory, demonstrating containerization, automation, and scripting proficiency. Key Features: Used a minimal base image (Alpine Linux) to create a lightweight Docker container. Installed Python in the container to execute a custom script automatically on launch. Script capabilities included: Listing all .txt files in /home/data Counting words in each file (IF.txt and Limerick-1.txt) Calculating total word count across files Identifying the top 3 most frequent words in IF.txt with word counts Retrieving the host machine's IP address Writing all outputs to /home/output/result.txt Configured the container to: Run automatically upon execution Print output to the console from result.txt Exit cleanly after execution Shared project image (.tar file) and source code via GitHub
Skills: Docker · Python Programming · Linux & Shell Commands · Automation & Scripting · Software Packaging · Git & Version Control · IP Address Retrieval · Alpine Linux · Shell Scripting · File I/O</p>
            <h3>GCP</h3>
            <p>Developed a cloud-based College Inquiry Chatbot using Google Cloud Platform (GCP) to answer student queries about college details such as academic programs, tuition fees, housing options, and campus facilities. The chatbot collects user information and provides real-time responses to frequently asked questions.
Key Features:
Implemented chatbot functionality using Dialogflow (SaaS) for natural language understanding and dialogue management.
Deployed the chatbot interface via Google App Engine (PaaS) and/or Google Compute Engine (IaaS).
Designed a user-friendly interface to collect user first name, last name, and email before initiating chat.

Configured responses for student FAQs including:
Availability of majors like Computer Science
In-state tuition details
Sports and extracurricular offerings (e.g., football team)
On-campus housing options
Displayed chatbot summary including user input and creator info at session end.
Developed a cloud-based College Inquiry Chatbot using Google Cloud Platform (GCP) to answer student queries about college details such as academic programs, tuition fees, housing options, and campus facilities. The chatbot collects user information and provides real-time responses to frequently asked questions. Key Features: Implemented chatbot functionality using Dialogflow (SaaS) for natural language understanding and dialogue management. Deployed the chatbot interface via Google App Engine (PaaS) and/or Google Compute Engine (IaaS). Designed a user-friendly interface to collect user first name, last name, and email before initiating chat. Configured responses for student FAQs including: Availability of majors like Computer Science In-state tuition details Sports and extracurricular offerings (e.g., football team) On-campus housing options Displayed chatbot summary including user input and creator info at session end.
Skills: Google Cloud Platform (GCP) · Google Dialogflow · Google App Engine · Google Compute Engine · Cloud Deployment · Flask · Natural Language Processing · Web Application Development · SaaS · User Interaction Design</p>
            <h3>Big Data with PySpark using Anaconda & Jupyter notebook</h3>
            <p>Performed large-scale weather data analysis using PySpark in a local environment via Anaconda and Jupyter Notebook. Leveraged distributed computing to process and analyze extensive weather datasets efficiently.

Key Tasks & Accomplishments:

Identified hottest day per year (2010–2022) with associated station info using MAX temperature.

Determined the coldest January day across all years, including date and location.

Extracted maximum and minimum precipitation data for the year 2015.

Calculated percentage of missing values in wind gust data for 2019 using null handling in PySpark.

Computed mean, median, mode, and standard deviation of monthly temperature in 2020, applying Spark SQL functions and window operations for statistical insights.

Outcome:
Successfully demonstrated the power of PySpark for real-world big data processing, statistical computation, and handling of missing values in large weather datasets.
Performed large-scale weather data analysis using PySpark in a local environment via Anaconda and Jupyter Notebook. Leveraged distributed computing to process and analyze extensive weather datasets efficiently. Key Tasks & Accomplishments: Identified hottest day per year (2010–2022) with associated station info using MAX temperature. Determined the coldest January day across all years, including date and location. Extracted maximum and minimum precipitation data for the year 2015. Calculated percentage of missing values in wind gust data for 2019 using null handling in PySpark. Computed mean, median, mode, and standard deviation of monthly temperature in 2020, applying Spark SQL functions and window operations for statistical insights. Outcome: Successfully demonstrated the power of PySpark for real-world big data processing, statistical computation, and handling of missing values in large weather datasets.
Skills: PySpark · Apache Spark · Big Data Analytics · Jupyter Notebook · Anaconda · Python (Programming Language) · Data Analysis · Data Cleaning · Statistical Analysis · Data Aggregation · Spark SQL · Distributed Computing · Exploratory Data Analysis (EDA) · Data Visualization · Time Series Data Analysis · Missing Data Handling · Weather Data Analytics · Data Wrangling</p>
            <h3>Exploratory Data Analysis using Databricks</h3>
            <p>Conducted an exploratory data analysis (EDA) project on real-world crime datasets from three U.S. cities (Dallas, Chicago, and one additional city) using Apache Spark on Databricks to derive insights from large-scale data.

Utilized Databricks Notebooks and PySpark to perform scalable data manipulation, transformation, and filtering on crime datasets.

Aggregated and analyzed crime incident types, with a focus on robbery and homicide rates by city and by month.

Generated statistical summaries and performed grouping operations to identify patterns in crime distribution, monthly trends, and demographic impacts.

Calculated robbery and homicide rates per capita using estimated population data and visualized the trends using customized time-series plots.

Applied string filtering, case-insensitive matching, and used Spark SQL functions such as lower(), like(), groupBy(), agg(), and join() to enrich and clean datasets.

Visualized key findings to highlight cities with the highest and lowest crime rates and developed insights for data-driven policy or law enforcement strategies.
Conducted an exploratory data analysis (EDA) project on real-world crime datasets from three U.S. cities (Dallas, Chicago, and one additional city) using Apache Spark on Databricks to derive insights from large-scale data. Utilized Databricks Notebooks and PySpark to perform scalable data manipulation, transformation, and filtering on crime datasets. Aggregated and analyzed crime incident types, with a focus on robbery and homicide rates by city and by month. Generated statistical summaries and performed grouping operations to identify patterns in crime distribution, monthly trends, and demographic impacts. Calculated robbery and homicide rates per capita using estimated population data and visualized the trends using customized time-series plots. Applied string filtering, case-insensitive matching, and used Spark SQL functions such as lower(), like(), groupBy(), agg(), and join() to enrich and clean datasets. Visualized key findings to highlight cities with the highest and lowest crime rates and developed insights for data-driven policy or law enforcement strategies.
Skills: Databricks · Apache Spark · PySpark · Big Data Analytics · Exploratory Data Analysis (EDA) · SQL · Data Wrangling · Data Visualization · Time-Series Analysis · Crime Data Analysis · Data Aggregation & Grouping · String Manipulation in Spark · Databricks Community Edition · Jupyter/Databricks Notebooks · Spark SQL · Python · PL/SQL</p>
            <h3>Data Science using Azure Cloud Technologies and Tools</h3>
            <p>As data volume, variety and velocity accelerate, organizations need to leverage modern data engineering. Every industry is being disrupted by data. In healthcare and life sciences, genomic data enable targeted drug discovery and personalized medicine. In financial services, alternative and third-party data provide increasingly vital investment signals. In retail and consumer packaged goods, demand forecasting, pricing, and inventory data deliver an order of magnitude efficiencies. Azure and Cloud Computing provide an opportunity to deploy Software application solutions easier, faster, better, and at scale.

Machine learning has gained significant importance in the retail sector owing to its capacity to analyze extensive datasets and derive valuable insights, thereby enhancing decision-making processes and improving customer experiences. Understanding the applications of ML models like linear regression, random forest, and gradient boosting algorithms is vital for accurately forecasting product demand, efficiently managing inventory levels, and optimizing pricing strategies in alignment with market demands.

For the final group project, we will explore real-world Retail data from 84.51°/Kroger (anonymized) to create an interactive Web Application in Azure Cloud. You will have the ability to focus on innovative problem-solving solutions using the Azure technology stack. The goal is to work on solutions that will make life easier for a Retail shopper. Be creative and put yourself into the shopper’s shoes. Focus on the core principle, "Make the Customer's Life Easier"
As data volume, variety and velocity accelerate, organizations need to leverage modern data engineering. Every industry is being disrupted by data. In healthcare and life sciences, genomic data enable targeted drug discovery and personalized medicine. In financial services, alternative and third-party data provide increasingly vital investment signals. In retail and consumer packaged goods, demand forecasting, pricing, and inventory data deliver an order of magnitude efficiencies. Azure and Cloud Computing provide an opportunity to deploy Software application solutions easier, faster, better, and at scale. Machine learning has gained significant importance in the retail sector owing to its capacity to analyze extensive datasets and derive valuable insights, thereby enhancing decision-making processes and improving customer experiences. Understanding the applications of ML models like linear regression, random forest, and gradient boosting algorithms is vital for accurately forecasting product demand, efficiently managing inventory levels, and optimizing pricing strategies in alignment with market demands. For the final group project, we will explore real-world Retail data from 84.51°/Kroger (anonymized) to create an interactive Web Application in Azure Cloud. You will have the ability to focus on innovative problem-solving solutions using the Azure technology stack. The goal is to work on solutions that will make life easier for a Retail shopper. Be creative and put yourself into the shopper’s shoes. Focus on the core principle, "Make the Customer's Life Easier"
Skills: Microsoft Azure · Cloud Deployment · Python (Programming Language) · pandas · Scikit-Learn · Matplotlib · Seaborn · SQL · HTML · Cascading Style Sheets (CSS) · JavaScript · Flask · Django · Machine Learning Models · Exploratory Data Analysis · Predictive Modeling · Data Visualization · Azure SQL · ETL Processes · Data Modeling · GitHub · Power BI · Tableau · Databricks
Skills: Microsoft Azure · Cloud Deployment · Python (Programming Language) · pandas · Scikit-Learn · Matplotlib · Seaborn · SQL · HTML · Cascading Style Sheets (CSS) · JavaScript · Flask · Django · Machine Learning Models · Exploratory Data Analysis · Predictive Modeling · Data Visualization · Azure SQL · ETL Processes · Data Modeling · GitHub · Power BI · Tableau · Databricks
</p>
        </div>
    </section>

    <!-- Skills Section -->
    <section id="skills">
        <h2>Skills</h2>
        <ul class="skills-list">
            <li>Azure</li> 
            <li>SQL</li>
            <li>Python</li>
            <li>Data Modeling</li>
            <li>Angular JS</li>
            <li>C#</li>
            <li>JavaScript</li>
            <li>jQuery</li>
            <li>HTML</li>
            <li>T-SQL</li>
            <li>Java</li>
            <li>Python</li>
            <li>SQL Server</li>
            <li>EF Core</li>
            <li>Visual Studio</li>
            <li>Team Foundation Server (TFS)</li>
            <li>GitHub</li>
            <li>Rally</li>
            <li>Web Services</li>
            <li>JSON</li>
            <li>XML</li>
            <li>Agile (Scrum)</li>
            <li>Test-Driven Development (TDD)</li>
            <li>Object-Oriented Programming (OOP)</li>
            <li>Automated deployments</li>
            <li>Continuous Integration/Continuous Delivery (CI/CD)</li>
            <li>Service-Oriented Architecture (SOA)</li>
            <li>API Development</li>
             <li>Claims and Underwriting systems knowledge</li>
            <li>AWS</li>
            <li>Azure</li>

            <!-- Add more skills as needed -->
        </ul>
    </section>

    <div>
    <!-- Certification section -->
    <section id="Certification">
        <h2>Certification</h2>
        <p>HCL Developer of Machine learning</p>
        <p>Python Developer Certification</p>
        <p>NPTEL Certification from IIT Kanpur India</p>
        <p>Azure Fundamentals from Microsoft</p>
    </div>
    </section>


    <!-- Contact Section -->
    <section id="contact">
        <h2>Contact</h2>
        <p>If you'd like to get in touch, please reach out through <a href="https://www.linkedin.com/in/likhith-reddy-satti/">LinkedIn</a> or email me at <a href="mailto:Likhit99.com@gmail.com">Likhit99.com@gmail.com</a>.</p>
    </section>

    <!-- Footer -->
    <footer>
        <p>&copy; 2024 Likhithsainadhreddy Satti. All Rights Reserved.</p>
    </footer>
</body>
</html>
